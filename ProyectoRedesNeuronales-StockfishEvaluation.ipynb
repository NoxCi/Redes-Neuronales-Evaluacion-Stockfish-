{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos que se usan en este proyecto se pueden obtener en la siguiente carpeta de drive:\n",
    "https://drive.google.com/drive/folders/1JtJOwMGcV6QVgm3wcaJRUa6ImQ_C_-d3?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import chess # chess==1.3.0\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Procesamos los datos de entrada originales que estan en formato FEN para \n",
    "convertilos un vector de 64 enteros y las salidas originales las normalizamos.\n",
    "\"\"\"\n",
    "\n",
    "data_path = 'data/fen_to_stockfish_evaluation.csv'\n",
    "\n",
    "def process_data():\n",
    "    \"\"\" \n",
    "    Procesamos la informacion del .csv para convertir el tablero en formato\n",
    "    fen a un vector de 64 entradas     \n",
    "    \"\"\"\n",
    "    def converter(fen_board):\n",
    "        board = chess.Board(fen_board)\n",
    "        input_vec = []\n",
    "        for row in range(7,-1,-1):\n",
    "            for col in range(8):\n",
    "                piece = board.piece_at(chess.square(col,row))\n",
    "                \n",
    "                if piece is None:\n",
    "                    val = 0\n",
    "                else:\n",
    "                    val = piece.piece_type\n",
    "                    if not piece.color:\n",
    "                        val *= -1\n",
    "                input_vec.append(val)\n",
    "        return input_vec\n",
    "    \n",
    "    data_frame = pd.read_csv(data_path,names=['fen','eval'],converters={'fen':converter})\n",
    "    size = data_frame.shape[0]\n",
    "    array = data_frame.to_numpy()\n",
    "    fen = np.array(list(array[:,0]))\n",
    "    eval_ = array[:,1].reshape((size,1))\n",
    "    eval_ = (eval_-min(eval_))/(max(eval_)-min(eval_))\n",
    "    \n",
    "    array = np.zeros((size,65))\n",
    "    array[:,:-1] = fen\n",
    "    array[:,-1:] = eval_\n",
    "        \n",
    "    data_frame = pd.DataFrame(array)\n",
    "    \n",
    "    \n",
    "    data_frame.to_csv('processed_data_normalizada.csv', index=False)\n",
    "    \n",
    "process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creamos una subclase de Dataset, para poder obtener muestras de los datos\n",
    "de forma mas eficiente\n",
    "\"\"\"\n",
    "\n",
    "p_data_path = 'data/processed_data_normalizada.csv'\n",
    "\n",
    "class Data_set(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        fen = self.data_frame.iloc[idx, :-1]\n",
    "        fen = fen.to_numpy()\n",
    "        fen = fen.astype('int')\n",
    "        \n",
    "        eval_ = self.data_frame.iloc[idx, -1:]\n",
    "        eval_ = eval_.to_numpy()\n",
    "        eval_ = eval_.astype('float')\n",
    "        \n",
    "        sample = (torch.tensor(fen,dtype=torch.float,device=device),\n",
    "                  torch.tensor(eval_,dtype=torch.float,device=device))\n",
    "        \n",
    "\n",
    "        return sample\n",
    "\n",
    "# Cargamos los datos en p_data_path\n",
    "dataset = Data_set(p_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (h1): Linear(in_features=64, out_features=1024, bias=True)\n",
       "  (h2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (h3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (outlayer): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  (activation): ELU(alpha=1.0)\n",
       "  (batchNormalization): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (hiddenNormalization): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.h1 = nn.Linear(64,1024)\n",
    "        self.h2 = nn.Linear(1024,1024)\n",
    "        self.h3 = nn.Linear(1024,1024)\n",
    "        self.outlayer = nn.Linear(1024,1)\n",
    "        \n",
    "        self.activation = nn.ELU()\n",
    "        self.batchNormalization = nn.BatchNorm1d(64)\n",
    "        self.hiddenNormalization = nn.BatchNorm1d(1024)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \n",
    "        out = self.activation(self.h1(batch))\n",
    "        out = self.hiddenNormalization(out)\n",
    "        out = self.activation(self.h2(out))\n",
    "        out = self.hiddenNormalization(out)\n",
    "        out = self.activation(self.h3(out))\n",
    "        out = self.hiddenNormalization(out)\n",
    "        out = self.activation(self.outlayer(out))\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def train(self, epochs, data, criterion, optimizer, batch_size, data_size):\n",
    "        errors = np.zeros(epochs)\n",
    "        num_batches = data_size // batch_size\n",
    "        print(f'Numero de lotes por epoch {num_batches}, tamaño de los lotes {batch_size}')\n",
    "        time_stamp = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            print(f'epoch {epoch}/{epochs}')\n",
    "            running_loss = 0\n",
    "            for batch_i in range(num_batches):\n",
    "                if batch_i % int(num_batches*0.1) == 0:\n",
    "                    print(f'{int((batch_i*100)/num_batches)}% epoch')\n",
    "                \n",
    "                batch = data[batch_i*batch_size : batch_i*batch_size + batch_size]\n",
    "                inputs, vals = batch\n",
    "                inputs = self.batchNormalization(inputs)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs,vals)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()*inputs.size(0)\n",
    "            epoch_loss = running_loss / data_size\n",
    "            print(f'epoch_loss: {epoch_loss}')\n",
    "            errors[epoch] = epoch_loss\n",
    "            torch.save(net.state_dict(),\"./net.pth\")\n",
    "            \n",
    "        print(f'time(min): {(time.time()-time_stamp)/60}')\n",
    "        \n",
    "        if epochs > 1:\n",
    "            X_axis = np.arange(epochs)\n",
    "            plt.figure(1)\n",
    "            plt.plot(X_axis, errors)\n",
    "            \n",
    "# Creamos una red nueva e intentamos cargar los pesos de una guardada si esta existe\n",
    "net=Net() \n",
    "try:\n",
    "    std_dict = torch.load('net.pth')\n",
    "    net.load_state_dict(std_dict)\n",
    "    print('Red cargada')\n",
    "except:\n",
    "    pass\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de lotes por epoch 13757, tamaño de los lotes 256\n",
      "epoch 0/1\n",
      "0% epoch\n",
      "9% epoch\n",
      "19% epoch\n",
      "29% epoch\n",
      "39% epoch\n",
      "49% epoch\n",
      "59% epoch\n",
      "69% epoch\n",
      "79% epoch\n",
      "89% epoch\n",
      "99% epoch\n",
      "epoch_loss: 0.05351503683840172\n",
      "time(min): 22.0408526579539\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "params = net.parameters()\n",
    "optimizer = optim.SGD(params, lr=0.0001, momentum=0.1, nesterov=True)\n",
    "criterion = nn.MSELoss()\n",
    "batch_size = 256\n",
    "# train_data_size = 256*1000\n",
    "train_data_size = int(len(dataset)*0.8)\n",
    "net.train(1,dataset,criterion,optimizer,batch_size,train_data_size)\n",
    "\n",
    "torch.save(net.state_dict(),\"./net.pth\")\n",
    "print('Model Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red cargada\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (h1): Linear(in_features=64, out_features=1024, bias=True)\n",
       "  (h2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (h3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (outlayer): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  (activation): ELU(alpha=1.0)\n",
       "  (batchNormalization): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (hiddenNormalization): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=Net()\n",
    "try:\n",
    "    std_dict = torch.load('./net.pth')\n",
    "    net.load_state_dict(std_dict)\n",
    "    print('Red cargada')\n",
    "except:\n",
    "    print('No se pudo cargar la red')\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision en conjunto de entrenamiento: 13.8125%\n",
      "Precision en conjunto de prueba: 12.4046%\n",
      "Precision en conjunto de validacion: 12.4046%\n"
     ]
    }
   ],
   "source": [
    "# Verificacion del modelo\n",
    "\n",
    "delta = 0.02\n",
    "\n",
    "def contarCorrectas(net,batch,vals):\n",
    "    '''Dado un batch y sus etiquetas, cuenta el numero de respuestas\n",
    "    correctas de una red, el parametro func aplica una modificacion al \n",
    "    tensor que contiene los datos'''\n",
    "    \n",
    "    salidas=net(batch)\n",
    "    cantidadCorrectas=(torch.abs(salidas-vals) < delta).sum()\n",
    "    return cantidadCorrectas\n",
    "    \n",
    "def calcularPrecisionGlobal(net,data_set,batch_size,range_):\n",
    "    '''Calcula la precision de una red dado un data_loader,\n",
    "    recibe una funcion que transforma los datos en caso de ser necesario'''\n",
    "    correctas=0\n",
    "    num_batches = (range_[1]-range_[0]) // batch_size\n",
    "    for batch_i in range(num_batches):\n",
    "            batch = data_set[range_[0]+batch_i*batch_size : range_[0]+batch_i*batch_size + batch_size]\n",
    "            inputs, vals = batch\n",
    "            correctas+=contarCorrectas(net,inputs,vals)        \n",
    "    correctas=correctas.data.tolist()\n",
    "    return (100*correctas)/(num_batches*batch_size)\n",
    "\n",
    "train_data_size = int(len(dataset)*0.8)\n",
    "test_data_size = int(len(dataset)*0.1)\n",
    "val_data_size = int(len(dataset)*0.1)\n",
    "range_train = (0,256000)\n",
    "range_test = (train_data_size,train_data_size+test_data_size)\n",
    "range_val = (train_data_size+test_data_size, train_data_size+test_data_size+val_data_size)\n",
    "prec_train = calcularPrecisionGlobal(net,dataset,512,range_train)\n",
    "prec_test = calcularPrecisionGlobal(net,dataset,256,range_test)\n",
    "prec_val = calcularPrecisionGlobal(net,dataset,256,range_test)\n",
    "print(\"Precision en conjunto de entrenamiento: %.4f%%\"%(prec_train))\n",
    "print(\"Precision en conjunto de prueba: %.4f%%\"%(prec_test))\n",
    "print(\"Precision en conjunto de validacion: %.4f%%\"%(prec_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5302],\n",
      "        [0.5272],\n",
      "        [0.5411],\n",
      "        [0.5382],\n",
      "        [0.5520],\n",
      "        [0.5464],\n",
      "        [0.5473],\n",
      "        [0.5466],\n",
      "        [0.5451],\n",
      "        [0.5461]])\n",
      "tensor([[0.5567],\n",
      "        [0.1330],\n",
      "        [0.5260],\n",
      "        [0.3145],\n",
      "        [0.5900],\n",
      "        [0.7342],\n",
      "        [0.7721],\n",
      "        [0.2822],\n",
      "        [0.2686],\n",
      "        [0.6954]], grad_fn=<EluBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Muestra aleatoria\n",
    "\n",
    "i = random.randint(0,len(dataset)-10) \n",
    "\n",
    "m = net.batchNormalization\n",
    "print(dataset[i:i+10][1])\n",
    "print(net(m(dataset[i:i+10][0])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
